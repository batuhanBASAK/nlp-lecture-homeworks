{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from turkishnlp import detector\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated folder: dataset\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_folder():\n",
    "    folder_name = 'dataset'\n",
    "    # Check if the folder already exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs('dataset')  # Create the folder\n",
    "\n",
    "        os.makedirs('./dataset/syllable')\n",
    "        os.makedirs('./dataset/character')\n",
    "\n",
    "        os.makedirs('./dataset/character/train')\n",
    "        os.makedirs('./dataset/character/test')\n",
    "\n",
    "        os.makedirs('./dataset/syllable/train')\n",
    "        os.makedirs('./dataset/syllable/test')\n",
    "\n",
    "\n",
    "        print(f'Created folder: {folder_name}')\n",
    "    else:\n",
    "        print(f'Recreated folder: {folder_name}')\n",
    "        os.system('rm -rf ./dataset')\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "        os.makedirs('./dataset/syllable')\n",
    "        os.makedirs('./dataset/character')\n",
    "\n",
    "        os.makedirs('./dataset/character/train')\n",
    "        os.makedirs('./dataset/character/test')\n",
    "\n",
    "        os.makedirs('./dataset/syllable/train')\n",
    "        os.makedirs('./dataset/syllable/test')\n",
    "\n",
    "create_dataset_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = './wikipedia_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_to_token = {\n",
    "    ' ': '<space>',\n",
    "    '.': '<period>', \n",
    "    '\\n': '<new_line>', \n",
    "    '\\t': '<space>',\n",
    "    '!': '<exclamation_mark>',\n",
    "    '-': '<dash>',\n",
    "    '?': '<question_mark>',\n",
    "    '/': '<forward_slash>',\n",
    "    '\\\\': '<back_slash>',\n",
    "    '/': '<forward_slash>',\n",
    "    '\"': '<double_quotes>',\n",
    "    \"'\": '<single_quotes>',\n",
    "    \";\": '<semicolon>',\n",
    "    \":\": '<column>',\n",
    "    \"(\": '<open_paranthesis>',\n",
    "    \")\": '<close_paranthesis>',\n",
    "    \"[\": '<open_square_bracket>',\n",
    "    \"]\": '<close_square_bracket>',\n",
    "    \"{\": '<open_curly_bracket>',\n",
    "    \"}\": '<close_curly_bracket>',\n",
    "    \"%\": '<percentage_symbol>'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "sentence_end_tokens = [\n",
    "    '<period>',\n",
    "    '<exclamation_mark>',\n",
    "    '<question_mark>'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_line function process given line and returns a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    # Define the regex patterns for words, digits, and punctuation\n",
    "    turkish_word_pattern = r'[a-zA-ZşŞıİğĞçÇöÖüÜ]+'\n",
    "    digit_pattern = r'0|[1-9][0-9]*'\n",
    "    \n",
    "    # Initialize the output list\n",
    "    token_list = []\n",
    "    \n",
    "    # Replace punctuation with tokens\n",
    "    line_with_tokens = line\n",
    "    for punc, token in punctuation_to_token.items():\n",
    "        line_with_tokens = line_with_tokens.replace(punc, f' {token} ')\n",
    "    \n",
    "    # Use regex to find all words, digits, and tokens\n",
    "    matches = re.findall(r'(' + turkish_word_pattern + r'|' + digit_pattern + r'|<[^>]+>)', line_with_tokens)\n",
    "\n",
    "\n",
    "    # each line starts with <start> symbol end ends with a <end> symbol\n",
    "    for match in matches:\n",
    "\n",
    "        if re.match(turkish_word_pattern, match):\n",
    "            token_list.append(match)\n",
    "        elif re.match(digit_pattern, match):\n",
    "            token_list.extend(match)  # Extend with individual digits\n",
    "        elif match in list(punctuation_to_token.values()):\n",
    "            token_list.append(match)\n",
    "\n",
    "\n",
    "    if '<new_line>' in token_list:\n",
    "        token_list.remove('<new_line>') # get rid of new line\n",
    "\n",
    "    output = []\n",
    "    output.append(\"<start>\")\n",
    "    output.extend(token_list)\n",
    "    output.append(\"<end>\")\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset For Syllable-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_turkish_nlp(data_folder=None):\n",
    "    \"\"\"\n",
    "    Loads Turkish NLP data from a local directory and initializes the TurkishNLP detector.\n",
    "    \n",
    "    Args:\n",
    "        data_folder (str): Path to the folder containing the TurkishNLP data files. \n",
    "                           If None, it defaults to '~/TRnlpdata/'.\n",
    "\n",
    "    Returns:\n",
    "        obj (TurkishNLP): Initialized TurkishNLP object with data loaded from local files.\n",
    "    \"\"\"\n",
    "    if data_folder is None:\n",
    "        data_folder = os.path.expanduser('~/TRnlpdata/')\n",
    "    \n",
    "    words_alt_path = os.path.join(data_folder, 'words_alt.pkl')\n",
    "    words_counted_path = os.path.join(data_folder, 'words_counted.pkl')\n",
    "    words_path = os.path.join(data_folder, 'words.pkl')\n",
    "\n",
    "    # Load the data from pickle files\n",
    "    def load_data(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    try:\n",
    "        words_alt = load_data(words_alt_path)\n",
    "        words_counted = load_data(words_counted_path)\n",
    "        words = load_data(words_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please check if the file exists in the specified directory.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize TurkishNLP detector and set the loaded data\n",
    "    obj = detector.TurkishNLP()\n",
    "    obj.words_alt = words_alt\n",
    "    obj.words_counted = words_counted\n",
    "    obj.words = words\n",
    "\n",
    "    return obj\n",
    "\n",
    "obj = initialize_turkish_nlp('./TRnlpdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabify_words(input_list):\n",
    "    output = []\n",
    "    \n",
    "    for item in input_list:\n",
    "        # Check if the item is a digit or a punctuation token\n",
    "        if item in punctuation_to_token.values() or item.isdigit():\n",
    "            output.append(item)  # Append the item as is\n",
    "        elif item in ['<start>', '<end>']:\n",
    "            output.append(item)  # Append the item as is\n",
    "        else:\n",
    "            # Syllabify the word\n",
    "            syllables = obj.syllabicate_sentence(item)\n",
    "            # Flatten the list of syllables and add to output\n",
    "            output.extend([syllable for sublist in syllables for syllable in sublist])\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_for_syllable_data(input_file_path, output_file_path):\n",
    "    print(f'Processing the file `{input_file_path}` and writing output data to file `{output_file_path}`')\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "\n",
    "            line = line.lower()\n",
    "\n",
    "            if len(line.split()) == 0:\n",
    "                continue\n",
    "\n",
    "            # Check if the line consists of HTML tags\n",
    "            if re.match(r'^\\s*<.*?>\\s*$', line):\n",
    "                continue  # Skip this line\n",
    "            \n",
    "            # Process the line to get words, punctuation, and digits\n",
    "            processed_line = process_line(line)  # Using your process_line function\n",
    "            # Syllabify the processed line\n",
    "            syllabified_line = syllabify_words(processed_line)  # Using your syllabify_words function\n",
    "\n",
    "            # Write the result to the output file, joined by spaces, followed by a newline\n",
    "            outfile.write(' '.join(syllabified_line) + '\\n')  # Adding a newline at the end of each line\n",
    "    print('The preprocessing has been completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the file `./wikipedia_data.txt` and writing output data to file `./dataset/syllable_data.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4547965it [03:23, 22335.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preprocessing has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join('./dataset', 'syllable_data.txt')\n",
    "process_file_for_syllable_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset for Character-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_characters(processed_line):\n",
    "    output_list = []\n",
    "\n",
    "    for token in processed_line:\n",
    "        if token in list(punctuation_to_token.values()):\n",
    "            output_list.append(token)\n",
    "        elif token in ['<start>', '<end>']:\n",
    "            output_list.append(token)\n",
    "        elif token.isdigit():\n",
    "            output_list.extend(token)  # Add each digit as a separate character\n",
    "        else:\n",
    "            output_list.extend(list(token))  # Add each character of the word\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_for_character_data(input_file_path, output_file_path):\n",
    "    print(f'Processing the file `{input_file_path}` for character data and writing output data to file `{output_file_path}`')\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "\n",
    "            line = line.lower()\n",
    "\n",
    "            if len(line.split()) == 0:\n",
    "                continue\n",
    "\n",
    "            # Check if the line consists of HTML tags\n",
    "            if re.match(r'^\\s*<.*?>\\s*$', line):\n",
    "                continue  # Skip this line\n",
    "            \n",
    "            # Process the line to get words, punctuation, and digits\n",
    "            processed_line = process_line(line)  # Using your process_line function\n",
    "            # Convert processed line to characters\n",
    "            character_data = process_characters(processed_line)\n",
    "            \n",
    "\n",
    "            # Write the result to the output file, joined by spaces, followed by a newline\n",
    "            outfile.write(' '.join(character_data) + '\\n')  # Adding a newline at the end of each line\n",
    "    \n",
    "    print('The character data processing has been completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the file `./wikipedia_data.txt` for character data and writing output data to file `./dataset/character_data.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4547965it [02:18, 32928.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character data processing has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join('./dataset', 'character_data.txt')\n",
    "process_file_for_character_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Dataset Files Create Unigram, Bigram, and Trigram Dataset Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_generator(infile: str, outfile: str):\n",
    "    print(f'Creating the unigram from file `{infile}` and saving it in file `{outfile}`')\n",
    "    with open(infile, 'r') as infile, open(outfile, 'w') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            line_splitted = line.split()\n",
    "            for token in line_splitted:\n",
    "                s = token + '\\n'\n",
    "                outfile.write(s)\n",
    "\n",
    "    print('Creating the unigram has been completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the unigram from file `./dataset/character_data.txt` and saving it in file `./dataset/character_unigram.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3922529it [00:31, 124038.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the unigram has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_generator('./dataset/character_data.txt', './dataset/character_unigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the unigram from file `./dataset/syllable_data.txt` and saving it in file `./dataset/syllable_unigram.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1736199it [00:22, 75514.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the unigram has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_generator('./dataset/syllable_data.txt', './dataset/syllable_unigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_generator(infile: str, outfile: str):\n",
    "    print(f'Creating the bigram from file `{infile}` and saving it in file `{outfile}`')\n",
    "    with open(infile, 'r') as infile, open(outfile, 'w') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            line_splitted = line.split()\n",
    "            for i in range(len(line_splitted) - 1):\n",
    "                w0 = line_splitted[i]\n",
    "                w1 = line_splitted[i+1]\n",
    "                s = w0 + ' ' + w1 + '\\n'\n",
    "                outfile.write(s)\n",
    "\n",
    "    print('Creating the bigram has been completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bigram from file `./dataset/character_data.txt` and saving it in file `./dataset/character_bigram.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3922529it [00:53, 73197.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bigram has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_generator('./dataset/character_data.txt', './dataset/character_bigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bigram from file `./dataset/syllable_data.txt` and saving it in file `./dataset/syllable_bigram.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1736199it [00:36, 48222.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bigram has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_generator('./dataset/syllable_data.txt', './dataset/syllable_bigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_generator(infile: str, outfile: str):\n",
    "    print(f'Creating the trigram from file `{infile}` and saving it in file `{outfile}`')\n",
    "    with open(infile, 'r') as infile, open(outfile, 'w') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            line_splitted = line.split()\n",
    "            for i in range(len(line_splitted) - 2):\n",
    "                w0 = line_splitted[i]\n",
    "                w1 = line_splitted[i+1]\n",
    "                w2 = line_splitted[i+2]\n",
    "                s = w0 + ' ' + w1 + ' ' + w2 + '\\n'\n",
    "                outfile.write(s)\n",
    "\n",
    "    print('Creating the trigram has been completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the trigram from file `./dataset/character_data.txt` and saving it in file `./dataset/character_trigram.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3922529it [01:13, 53288.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the trigram has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_generator('./dataset/character_data.txt', './dataset/character_trigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the trigram from file `./dataset/syllable_data.txt` and saving it in file `./dataset/syllable_trigram.txt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1736199it [00:51, 33685.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the trigram has been completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_generator('./dataset/syllable_data.txt', './dataset/syllable_trigram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split N-Gram Dataset to Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_samples(infile):\n",
    "    count = 0\n",
    "    with open(infile, 'r') as file:\n",
    "        for line in file:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def train_test_split_n_gram(infile, train_outfile, test_outfile, train_ratio=0.95):\n",
    "    total_number_of_samples = count_number_of_samples(infile)\n",
    "    train_size = int(total_number_of_samples * train_ratio)\n",
    "    \n",
    "    with open(infile, 'r') as infile, open(train_outfile, 'w') as train_outfile, open(test_outfile, 'w') as test_outfile:\n",
    "        for i, line in tqdm(enumerate(infile)):\n",
    "            if i <= train_size:\n",
    "                train_outfile.write(line)\n",
    "            else:\n",
    "                test_outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395631701it [00:55, 7111028.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split_n_gram('./dataset/character_unigram.txt', \n",
    "                        './dataset/character/train/unigram.txt', \n",
    "                        './dataset/character/test/unigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391709172it [00:59, 6634821.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split_n_gram('./dataset/character_bigram.txt', \n",
    "                        './dataset/character/train/bigram.txt', \n",
    "                        './dataset/character/test/bigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "387786643it [01:03, 6094700.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split_n_gram('./dataset/character_trigram.txt', \n",
    "                        './dataset/character/train/trigram.txt', \n",
    "                        './dataset/character/test/trigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209175035it [00:33, 6197672.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split_n_gram('./dataset/syllable_unigram.txt', \n",
    "                        './dataset/syllable/train/unigram.txt', \n",
    "                        './dataset/syllable/test/unigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "207438836it [00:36, 5659802.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split_n_gram('./dataset/syllable_bigram.txt', \n",
    "                        './dataset/syllable/train/bigram.txt', \n",
    "                        './dataset/syllable/test/bigram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205702637it [00:40, 5077890.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_split_n_gram('./dataset/syllable_trigram.txt', \n",
    "                        './dataset/syllable/train/trigram.txt', \n",
    "                        './dataset/syllable/test/trigram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Uncessary Files from Dataset Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"rm -rf ./dataset/character_data.txt\")\n",
    "os.system(\"rm -rf ./dataset/character_unigram.txt\")\n",
    "os.system(\"rm -rf ./dataset/character_bigram.txt\")\n",
    "os.system(\"rm -rf ./dataset/character_trigram.txt\")\n",
    "\n",
    "os.system(\"rm -rf ./dataset/syllable_data.txt\")\n",
    "os.system(\"rm -rf ./dataset/syllable_unigram.txt\")\n",
    "os.system(\"rm -rf ./dataset/syllable_bigram.txt\")\n",
    "os.system(\"rm -rf ./dataset/syllable_trigram.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
